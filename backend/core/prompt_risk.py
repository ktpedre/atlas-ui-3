"""
Prompt injection risk heuristics and structured logging.

Scope: lightweight, configurable thresholds; used for user input and RAG results.
"""

from __future__ import annotations

import base64
import json
import logging
import math
import os
import re
from collections import Counter
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


# Default thresholds; can be overridden by env vars
THRESHOLDS = {
    "low": int(os.getenv("PI_THRESHOLD_LOW", "30")),
    "medium": int(os.getenv("PI_THRESHOLD_MEDIUM", "50")),
    "high": int(os.getenv("PI_THRESHOLD_HIGH", "80")),
}


def calculate_prompt_injection_risk(message: str, *, mode: str = "general") -> Dict[str, object]:
    """
    Calculate a heuristic risk score for prompt injection attempts.

    Returns: { 'score': int, 'risk_level': str, 'triggers': list[str] }
    """
    score = 0
    triggers: List[str] = []

    msg_lower = (message or "").lower()
    msg_clean = re.sub(r"\s+", " ", (message or "").strip())

    # 1) Suspicious patterns
    patterns = {
        "override_instructions": (r"ignore\s+(previous|all|everything|above|prior)", 40),
        "disregard": (r"disregard\s+(previous|all|everything|above|prior)", 40),
        "new_instructions": (r"new\s+instructions?\s*:\s*", 35),
        "system_role": (r"\b(system|assistant|user)\s*:\s*", 30),
        "act_as": (r"act\s+as\s+(if\s+)?you\s+(are|were)", 25),
        "pretend": (r"pretend\s+(to\s+be|you\s+are)", 25),
        "role_change": (r"your?\s+(new\s+)?role\s+(is|now)", 30),
        "forget": (r"forget\s+(everything|all|previous)", 35),
        "override": (r"override\s+(previous|default|system)", 35),
        "jailbreak": (r"(jailbreak|developer\s+mode|god\s+mode)", 45),
    }
    for name, (pat, pts) in patterns.items():
        if re.search(pat, msg_lower):
            score += pts
            triggers.append(name)

    # 2) Encodings/obfuscation
    if _detect_encoding(message):
        score += 30
        triggers.append("encoding_detected")

    # 3) Statistical anomalies
    # Delimiter density (triple quotes, fences, etc.)
    delimiters = len(re.findall(r"[#*\-_=]{3,}|[\"\"\"''']{3,}", message or ""))
    if delimiters >= 3:
        score += 25
        triggers.append("excessive_delimiters")
    elif delimiters >= 1:
        score += 10

    # High entropy (possible encoded blob)
    if len(message or "") > 10:
        ent = _calculate_entropy(message)
        if ent > 4.5:
            score += 20
            triggers.append("high_entropy")

    # Excessive caps
    if len(message or "") > 20:
        caps_ratio = sum(1 for c in (message or "") if c.isupper()) / max(1, len(message or ""))
        if caps_ratio > 0.3:
            score += 15
            triggers.append("excessive_caps")

    # 4) Context-breaking attempts
    if (message or "").count("\n") > 5 or re.search(r"\s{10,}", message or ""):
        score += 15
        triggers.append("formatting_abuse")

    if re.search(r"(human|user|assistant):\s*\n", msg_lower):
        score += 25
        triggers.append("fake_conversation")

    if (len(message or "") > 50) and re.search(r"<[^>]+>.*</[^>]+>|[{}\[\]]", message or ""):
        score += 20
        triggers.append("structured_injection")

    # 5) Length penalty
    if len(message or "") > 1000:
        score += 15
        triggers.append("excessive_length")

    # Context-aware normalization (reduce false positives)
    if mode in ("code", "logs"):
        # Code/logs: braces, fences common; soften penalties
        if "excessive_delimiters" in triggers:
            score -= 10
        if "structured_injection" in triggers:
            score -= 10
        score = max(0, score)

    # Risk buckets
    if score >= THRESHOLDS["high"]:
        level = "high"
    elif score >= THRESHOLDS["medium"]:
        level = "medium"
    elif score >= THRESHOLDS["low"]:
        level = "low"
    else:
        level = "minimal"

    return {"score": int(score), "risk_level": level, "triggers": triggers}


def _detect_encoding(text: str) -> bool:
    clean = re.sub(r"\s", "", text or "")
    # Base64-like
    if len(clean) > 20 and len(clean) % 4 == 0 and re.match(r"^[A-Za-z0-9+/=]+$", clean or ""):
        try:
            base64.b64decode(clean, validate=True)
            return True
        except Exception:
            pass
    # Hex
    if re.match(r"^(0x)?[0-9a-fA-F]+$", clean or "") and len(clean) > 20:
        return True
    # Escape sequences
    if re.search(r"\\u[0-9a-fA-F]{4}|\\x[0-9a-fA-F]{2}", text or ""):
        return True
    # Zero-width/unusual unicode
    if re.search(r"[\u200B-\u200D\uFEFF\u2060]", text or ""):
        return True
    return False


def _calculate_entropy(text: str) -> float:
    if not text:
        return 0.0
    counts = Counter(text)
    n = len(text)
    ent = 0.0
    for c in counts.values():
        p = c / n
        ent -= p * math.log2(max(p, 1e-12))
    return ent


def log_high_risk_event(*, source: str, user: Optional[str], content: str, score: int, risk_level: str, triggers: List[str], extra: Optional[Dict[str, object]] = None) -> None:
    """Append a JSONL record for medium/high events to logs/security_high_risk.jsonl."""
    try:
        # Only log medium/high
        if risk_level not in ("medium", "high"):
            return
        base_dir = Path(__file__).resolve().parents[2]
        log_path = base_dir / "logs" / "security_high_risk.jsonl"
        log_path.parent.mkdir(parents=True, exist_ok=True)
        record = {
            "ts": datetime.utcnow().isoformat() + "Z",
            "type": "prompt_risk",
            "source": source,
            "user": user,
            "score": score,
            "risk_level": risk_level,
            "triggers": triggers,
        }
        if extra:
            record.update(extra)
        # include a small snippet only
        snippet = (content or "")
        if len(snippet) > 240:
            snippet = snippet[:240] + "â€¦"
        record["snippet"] = snippet
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    except Exception as e:
        logger.debug("Failed to write high risk log: %s", e)
